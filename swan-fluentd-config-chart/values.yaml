fluentd:
  plugins:
    - fluent-plugin-rewrite-tag-filter
    - fluent-plugin-out-http
    - fluent-plugin-grok-parser
    - fluent-plugin-concat
  output:
    producer: swan
    endpoint: http://monit-logs.cern.ch:10012/
    includeInternal: false
  parsingConfig: |
    ### Define Classes of logs ###
    <match kubernetes.**>
      @type rewrite_tag_filter
      capitalize_regex_backreference true
      # jupyter notebook logs
      <rule>
        key $.kubernetes.pod_name
        pattern ^(jupyter-|hub-|proxy-|cvmfs-prefetcher-)
        tag swan
      </rule>
      <rule>
        key $.kubernetes.pod_name
        pattern (.?)
        tag system
      </rule>
    </match>
    <match {swan}>
      @type rewrite_tag_filter
      capitalize_regex_backreference true
      # jupyter or jupyterhub custom swan metrics
      <rule>
        key $.log
        pattern (^.*?user: .*?, host: .*?, metric: .*?, value: .*$)
        tag swan.metrics
      </rule>
      <rule>
        key $.log
        pattern (.?)
        tag swan.logs
      </rule>
    </match>

    ### Define Parsing of Jupyter/JupyterHub/Metrics logs ###

    # jupyter containers parser
    <filter {swan.logs}>
      @type concat
      key log

      # parse long json logs (swan env)
      stream_identity_key container_id
      multiline_end_regexp /\n$/
      separator ""
    </filter>
    <filter {swan.logs}>
      @type concat
      key log

      # parse exceptions and multiline debug logs
      stream_identity_key container_id
      multiline_start_regexp /^(\S)/
      continuous_line_regext /^(\s)/
    </filter>
    <filter {swan.logs}>
      @type parser
      key_name log

      # keep original record data additionaly to parsed one
      reserve_data true
      emit_invalid_record_to_error true

      # grok parsing
      <parse>
        @type grok

        # keep unmatched logs
        grok_failure_key grok_failure

        <grok>
          # jupyterhub hub message
          pattern \[%{WORD:log_level} %{TIMESTAMP_ISO8601:log_time} %{DATA:log_source}\] %{GREEDYDATA:log_msg}
          time_format "%Y-%m-%d %H:%M:%S.%N"
          keep_time_key false
          time_key log_time
          timezone +0100
        </grok>

        <grok>
          # jupyterhub culler message
          pattern \[%{WORD:log_level} (?<log_time>%{YEAR}%{MONTHNUM}%{MONTHDAY} %{TIME}) %{DATA:log_source}\] %{GREEDYDATA:log_msg}
          time_format "%y%m%d %H:%M:%S"
          keep_time_key false
          time_key log_time
          timezone +0100
        </grok>

        <grok>
          # jupyterhub proxy message
          pattern (?<log_time>%{TIME}.%{DATA}) \[%{DATA:log_source}\] (.*?)(?<log_level>(debug|info|error|warn))([^$]*): %{GREEDYDATA:log_msg}
          time_format "%H:%M:%S.%N"
          keep_time_key false
          time_key log_time
          timezone +0000
        </grok>

        <grok>
          # jupyterhub generic message
          pattern %{GREEDYDATA:log_msg}
        </grok>
      </parse>
    </filter>

    # parse jupyter and jupyterhub metrics
    <filter {swan.metrics}>
      @type parser
      key_name log

      reserve_data true
      emit_invalid_record_to_error true

      # grok parsing
      <parse>
        @type grok

        # keep unmatched logs
        grok_failure_key grokfailure

        <grok>
          pattern \[%{WORD} %{TIMESTAMP_ISO8601:log_time} %{DATA}\] %{GREEDYDATA:log_msg}
          time_format "%Y-%m-%d %H:%M:%S.%N"
          keep_time_key false
          time_key log_time
          timezone +0100
        </grok>
      </parse>
    </filter>
    <filter {swan.metrics}>
      @type parser
      key_name log_msg

      reserve_data true
      emit_invalid_record_to_error true

      # grok parsing
      <parse>
        @type grok

        # keep unmatched logs
        grok_failure_key grokfailure

        <grok>
          # string type metric_key.metric_context.metric_type
          pattern user: %{DATA:_metric_user_}, host: %{DATA}, metric: (?<_metric_key_>(\w*))(\.)(?<_metric_context_>(.*)), value: %{GREEDYDATA:_metric_value_}
          time_format "%Y-%m-%d %H:%M:%S.%N"
          keep_time_key false
          time_key metric_time
          timezone +0100
        </grok>
        <grok>
          # string type metric_key
          pattern user: %{DATA:_metric_user_}, host: %{DATA}, metric: (?<_metric_key_>(.*))(?<_metric_context_>(.*)), value: %{GREEDYDATA:_metric_value_}
          time_format "%Y-%m-%d %H:%M:%S.%N"
          keep_time_key false
          time_key metric_time
          timezone +0100
        </grok>
      </parse>
    </filter>
    <filter {swan.metrics}>
      @type record_transformer
      enable_ruby
      <record>
        metric_user ${record['_metric_user_']}
        metric.${record['_metric_key_']}.context ${record['_metric_context_']}
        metric.${record['_metric_key_']}.value ${record['_metric_value_']}
      </record>
      remove_keys ["_metric_time_", "_metric_raw_msg_", "_metric_user_", "_metric_key_", "_metric_context_", "_metric_value_"]
    </filter>

    ### Define Producers ###

    <filter {swan.logs}>
      @type record_transformer
      enable_ruby
      <record>
        producer "#{ENV['OUTPUT_PRODUCER']}"
        type "swanqa"
        timestamp ${(time.to_f * 1000).to_i}
      </record>
      remove_keys ["log"]
    </filter>
    <filter {swan.metrics}>
      @type record_transformer
      enable_ruby
      <record>
        producer "#{ENV['OUTPUT_PRODUCER']}"
        type "metricsqa"
        timestamp ${(time.to_f * 1000).to_i}
      </record>
      remove_keys ["log"]
    </filter>
    <filter {system}>
      @type record_transformer
      enable_ruby
      <record>
        producer "#{ENV['OUTPUT_PRODUCER']}"
        type "system"
        timestamp ${time.to_i * 1000}
        log_msg ${record['log'].strip}
      </record>
      remove_keys ["log"]
    </filter>

    ### Define Output Endpoint ###

    # rescue errors to fluentd pod stdout for debugging
    <label @ERROR>
      <match {swan.logs,swan.metrics,system}>
        @type stdout
      </match>
    </label>

    # all correctly parsed, send to http endpoint
    <match {swan.logs,swan.metrics,system}>
      @type http
      endpoint_url    "#{ENV['OUTPUT_ENDPOINT']}"
      serializer      json
      http_method     post
    </match>
