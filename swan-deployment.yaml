# CVMFS - StorageClass and PVC
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: csi-cvmfs-swan-sft-cern-ch
provisioner: csi-cvmfsplugin
parameters:
  repository: sft.cern.ch
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: cvmfs-sft-cern-ch-pvc
  namespace: swan
spec:
  accessModes:
  - ReadOnlyMany
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-cvmfs-swan-sft-cern-ch
---
# Jupyterhub -- RoleBinding for SWAN namespace default service account, Deployment and NodePort
apiVersion: v1
kind: Secret
metadata:
  name: swan-certs
  namespace: swan
type: Opaque
data:
  boxed.key: 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
  boxed.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURBRENDQWVnQ0NRQ1Y4bGJrL1BOYTREQU5CZ2txaGtpRzl3MEJBUXNGQURCQ01Rc3dDUVlEVlFRR0V3SlkKV0RFVk1CTUdBMVVFQnd3TVJHVm1ZWFZzZENCRGFYUjVNUnd3R2dZRFZRUUtEQk5FWldaaGRXeDBJRU52YlhCaApibmtnVEhSa01CNFhEVEUzTURNd056RXhORFF5TjFvWERURTRNRE13TnpFeE5EUXlOMW93UWpFTE1Ba0dBMVVFCkJoTUNXRmd4RlRBVEJnTlZCQWNNREVSbFptRjFiSFFnUTJsMGVURWNNQm9HQTFVRUNnd1RSR1ZtWVhWc2RDQkQKYjIxd1lXNTVJRXgwWkRDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTTV4c2RmZwptcTR6MWZVdTVtai9zNDJWV29yMkw1MmEyQUFVaDBjOXV2dmMydlpGcFJFOHBZdCsyMWNMZkJqd2I1b0xZMURLCi9wMVMyQXpEZXZvK0hGSk01a05xaHBCMXUwK2pxVGQyakRCQklIb0FIblBmMG9EN1RnM2lGdnMyWWdVK2FPQ1kKeWVQTnhGdWFnd2tROWM3V3ljc0dEL3hsU2pVSWlnMlJzYVVyTmRiOXpkRjVmOTJSZmtjL3RPYnA1NFFmam5HVApnU2VMdmlXZC9wQlpVOG1qUmJRaHNIYnV6bkdDTUxUSU5jMzJMNGpOVDYzR3lVMnZ6ZDBOeGdCaHoxd3VaWFhLCjJtb1BsVzZLcHVqdXc0dkF0Vk9RQzBYYWx6RXRaOWhHU3N5WkQ4RW9nczZPMHlFMi9MeDFoT0pxeURGazIvMWUKQWR2TE1jZGdva2FkbEM4Q0F3RUFBVEFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBS056ZXJyMGpwTS92NFZoNQp1Yy8vU3B2eEszWjAyeU5TV2ZYU1RUL2RuL1kwRkI4dkJxdkszdjVDV1NXa0Rnejc3WEkvSmRSZDQzcEgzVDJUCnZGMDRoSnVkUTJESGhiZER0cjBZRXdvTlY1TE9QRHhiNnhNbkMvWFRnR09EZ2ZHdGkwWVVBbisycVVNU0J6UksKMGhMeFV6Q0ZnZHEyeFZaOGl0OFVBZm1BNUc4OUlHbDEvWXUxMlhON0h4OFBOMGdyVjBQQXJ5SDZCaVhROGNUYQpjcTAzS0N1ZExhOUluQ2ZyRm9DWjRnSC9TZE50ZXpzT3NFWXA2WHlkZ0xmVlY5V1pTUVNreVFkUUNiV2duYytLCkVmUGkrRzBHTXp6KzFLQkpzbnRTOTVqdUJBVE1ualA2OWZKaUZQcGRlY2VrZ0F2OW1rVmdvYklvdVVHMjJjRW8KbWZtWnhnPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
  boxed.csr: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ2h6Q0NBVzhDQVFBd1FqRUxNQWtHQTFVRUJoTUNXRmd4RlRBVEJnTlZCQWNNREVSbFptRjFiSFFnUTJsMAplVEVjTUJvR0ExVUVDZ3dUUkdWbVlYVnNkQ0JEYjIxd1lXNTVJRXgwWkRDQ0FTSXdEUVlKS29aSWh2Y05BUUVCCkJRQURnZ0VQQURDQ0FRb0NnZ0VCQU01eHNkZmdtcTR6MWZVdTVtai9zNDJWV29yMkw1MmEyQUFVaDBjOXV2dmMKMnZaRnBSRThwWXQrMjFjTGZCandiNW9MWTFESy9wMVMyQXpEZXZvK0hGSk01a05xaHBCMXUwK2pxVGQyakRCQgpJSG9BSG5QZjBvRDdUZzNpRnZzMllnVSthT0NZeWVQTnhGdWFnd2tROWM3V3ljc0dEL3hsU2pVSWlnMlJzYVVyCk5kYjl6ZEY1ZjkyUmZrYy90T2JwNTRRZmpuR1RnU2VMdmlXZC9wQlpVOG1qUmJRaHNIYnV6bkdDTUxUSU5jMzIKTDRqTlQ2M0d5VTJ2emQwTnhnQmh6MXd1WlhYSzJtb1BsVzZLcHVqdXc0dkF0Vk9RQzBYYWx6RXRaOWhHU3N5WgpEOEVvZ3M2TzB5RTIvTHgxaE9KcXlERmsyLzFlQWR2TE1jZGdva2FkbEM4Q0F3RUFBYUFBTUEwR0NTcUdTSWIzCkRRRUJDd1VBQTRJQkFRQVBzSDB0TGd6TzRjMERDd296Uzl5Y2FmTVY5QWRDYkNIdTJHL3BPdGhnaStWNkY2aTcKMURLMWRuelpXWjhFQVEwaW4wWkFtdlVIVVRsWEdGQ3B5b2hKbGtnczdpd3FRT3A3UjNiQnZhS0hxQitCMWpTZApoSjlwaElzSkFnVXFqVXE2ei9oUDV5RU40OW43YWIreWEzREp5Zm1iR2YxSmNVOC9COE5Hc0JBLzBJc0NRa2NxCmFFTFVncDFCWlg3TFpwSlFwaUVHWnBCME1DUzMvdUV0Z2EyQTJQZHEyWnlDb1NOV3d5dEFvN2lOa3JZdG4rd1AKa1lZWldCZkFjZGZNOVNQaVBnRXRmNnJzRWFBR0J4KzVicDBma0Rtbm1RUE9OZ2ZhT211UEMzZk9XRGdTMmtSQQowZnVRWFU2MjlFOEdpUEQ2QXZCemptcTc1M0VVVzZNanlUQkQKLS0tLS1FTkQgQ0VSVElGSUNBVEUgUkVRVUVTVC0tLS0tCg==
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  creationTimestamp: null
  name: add-on-cluster-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: default
  namespace: swan
---
apiVersion: v1
kind: Service
metadata:
  name: &name swan
  namespace: &namespace swan
spec:
  type: NodePort
  selector:
    app: *name
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 80
    nodePort: 30080
  - name: https
    protocol: TCP
    port: 443
    targetPort: 443
    nodePort: 30443
---
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: &name swan
  namespace: &namespace swan
  labels:
    app: *name
spec:
  replicas: 1 
  template:
    metadata:
      labels:
        app: *name
        name: *name
    spec:
      terminationGracePeriodSeconds: 60
      containers:
      - name: jupyterhub
        #image: "gitlab-registry.cern.ch/swan/docker-images/jupyterhub:v1.8"
        # dev image
        image: "gitlab-registry.cern.ch/db/spark-service/docker-registry/jupyterhub:1.0"
        imagePullPolicy: Always
        ports:
        - name: http
          protocol: TCP
          containerPort: 80
        - name: https
          protocol: TCP
          containerPort: 443
        volumeMounts:
        - name: &vm_swan-certs swan-certs
          mountPath: /etc/boxed/certs
        - name: &vm_cvmfs-sft-cern-ch cvmfs-sft-cern-ch
          mountPath: /cvmfs/sft.cern.ch
        - name: &vm_swan-config swan-config
          mountPath: /srv/jupyterhub/jupyterhub_form.complete.html
          subPath: jupyterhub_form.complete.html
        - name: &vm_swan-config swan-config
          mountPath: /root/jupyterhub_config/kubespawner.py
          subPath: kubespawner.py
        - name: &vm_swan-tokens swan-tokens
          mountPath: /srv/jupyterhub/private/eos-token.sh
          subPath: eos-token.sh
        env:
        - name: PODINFO_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: PODINFO_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: PODINFO_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: PODINFO_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: PODINFO_NODE_IP
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
        - name: THIS_CONTAINER
          value: "JUPYTERHUB"
        - name: DEPLOYMENT_TYPE
          value: "kubespawner"
        - name: SPAWNER_FORM
          value: "complete"
        - name: HTTP_PORT
          value: "80"
        - name: HTTPS_PORT
          value: "443"
        ### JupyterHub configuration:
        - name: AUTH_TYPE
          value: "local" #FIXME: LDAP, but this should be OAuth2 from upstream JupyterHub
        - name: CONTAINER_IMAGE
          value: "gitlab-registry.cern.ch/swan/docker-images/systemuser:v5.1.1"
        - name: NAMESPACE
          value: *namespace
        ### LDAP parameters
        - name: LDAP_URI
          value: "ldap://ldap.swan.svc.cluster.local"
        - name: LDAP_PORT
          value: "389"
        - name: LDAP_BASE_DN
          value: "dc=example,dc=org"
        - name: LDAP_BIND_DN
          value: "cn=readuser,dc=example,dc=org"
        - name: LDAP_BIND_PASSWORD
          value: "readuser"
        ### Mounts for single-user containers
        - name: CVMFS_FOLDER
          value: "/cvmfs"
        - name: EOS_USER_PATH
          value: "/eos/home"
      volumes:
      - name: *vm_swan-certs
        secret:
          secretName: swan-certs
          defaultMode: 420 # 0644 perm
      - name: *vm_swan-config
        configMap:
          name: swan-config
          items:
          - key: kubespawner.py
            path: kubespawner.py
          - key: jupyterhub_form.complete.html
            path: jupyterhub_form.complete.html
      - name: *vm_swan-tokens
        secret:
          secretName: swan-tokens
          items:
          - key: eos-token.sh
            path: eos-token.sh
          defaultMode: 356 # 0544 perm
      - name: *vm_cvmfs-sft-cern-ch
        persistentVolumeClaim:
          claimName: cvmfs-sft-cern-ch-pvc
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: &name swan-config
  namespace: &namespace swan
data:
  kubespawner.py: |-
    ###
    # Remember to authorize the pod where JupyterHub runs to access the API
    # of the cluster and to list pods in the namespace
    #
    # As temporary workaround:
    # kubectl create clusterrolebinding add-on-cluster-admin --clusterrole=cluster-admin --serviceaccount=boxed:default
    ###

    # Configuration file for JupyterHub
    import os
    import pwd
    import socket
    import subprocess
    from kubernetes import client
    from kubernetes.client.rest import ApiException

    ### VARIABLES ###
    # Get configuration parameters from environment variables
    CVMFS_FOLDER            = os.environ['CVMFS_FOLDER']
    EOS_USER_PATH           = os.environ['EOS_USER_PATH']
    CONTAINER_IMAGE         = os.environ['CONTAINER_IMAGE']
    LDAP_URI                = os.environ['LDAP_URI']
    LDAP_PORT               = os.environ['LDAP_PORT']
    LDAP_BASE_DN            = os.environ['LDAP_BASE_DN']
    NAMESPACE               = os.environ['PODINFO_NAMESPACE']
    SERVER_HOSTNAME         = os.environ['PODINFO_NODE_NAME']

    c = get_config()

    ### Configuration for JupyterHub ###
    # JupyterHub runtime configuration
    jupyterhub_runtime_dir = '/srv/jupyterhub/jupyterhub_data/'
    os.makedirs(jupyterhub_runtime_dir, exist_ok=True)
    c.JupyterHub.cookie_secret_file = os.path.join(jupyterhub_runtime_dir, 'cookie_secret')
    c.JupyterHub.db_url = os.path.join(jupyterhub_runtime_dir, 'jupyterhub.sqlite')

    # Resume previous state if the Hub fails
    c.JupyterHub.cleanup_proxy = True      # Kill the proxy if the hub fails
    c.JupyterHub.cleanup_servers = False    # Do not kill single-user's servers (SQLite DB must be on persistent storage)

    # Logging
    c.JupyterHub.log_level = 'DEBUG'
    c.Spawner.debug = True
    c.LocalProcessSpawner.debug = True

    # Add SWAN look&feel
    c.JupyterHub.template_paths = ['/srv/jupyterhub/jh_gitlab/templates']
    c.JupyterHub.logo_file = '/usr/local/share/jupyterhub/static/swan/logos/logo_swan_cloudhisto.png'

    # Reach the Hub from outside
    c.JupyterHub.ip = "0.0.0.0"     # Listen on all IPs for HTTP traffic when in Kubernetes
    c.JupyterHub.port = 8000	# You may end up in detecting the wrong IP address due to:
                                #       - Kubernetes services in front of Pods (headed//headless//clusterIPs)
                                #       - hostNetwork used by the JupyterHub Pod

    c.JupyterHub.cleanup_servers = False
    # Use local_home set to true to prevent calling the script that updates EOS tickets
    c.JupyterHub.services = [
        {
            'name': 'cull-idle',
            'admin': True,
            'command': 'python3 /srv/jupyterhub/jh_gitlab/scripts/cull_idle_servers.py --cull_every=600 --timeout=14400 --local_home=True --cull_users=True'.split(),
        }
    ]

    # Reach the Hub from Jupyter containers
    # NOTE: The Hub IP must be known and rechable from spawned containers
    # 	Leveraging on the FQDN makes the Hub accessible both when the JupyterHub Pod
    #	uses the Kubernetes overlay network and the host network
    try:
      hub_ip = socket.gethostbyname(socket.getfqdn())
    except:
      print ("WARNING: Unable to identify iface IP from FQDN")
      s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
      s.connect(("8.8.8.8", 80))
      hub_ip = s.getsockname()[0]
    hub_port = 8080
    c.JupyterHub.hub_ip = hub_ip
    c.JupyterHub.hub_port = hub_port
    c.KubeSpawner.hub_connect_ip = hub_ip
    c.KubeSpawner.hub_connect_port = hub_port

    # Proxy
    # Wrap the start of the proxy to allow bigger headers in nodejs
    c.ConfigurableHTTPProxy.command = '/srv/jupyterhub/jh_gitlab/scripts/start_proxy.sh'

    # Load the list of users with admin privileges and enable access
    admins = set(open(os.path.join(os.path.dirname(__file__), 'adminslist'), 'r').read().splitlines())
    c.Authenticator.admin_users = admins
    c.JupyterHub.admin_access = True

    ### User Authentication ###
    if ( os.environ['AUTH_TYPE'] == "shibboleth" ):
        print ("Authenticator: Using user-defined authenticator")
        c.JupyterHub.authenticator_class = '%%%SHIBBOLETH_AUTHENTICATOR_CLASS%%%'
        # %%% Additional SHIBBOLETH_AUTHENTICATOR_CLASS parameters here %%% #

    elif ( os.environ['AUTH_TYPE'] == "local" ):
        print ("Authenticator: Using LDAP")
        c.JupyterHub.authenticator_class = 'ldapauthenticator.LDAPAuthenticator'
        c.LDAPAuthenticator.server_address = LDAP_URI
        c.LDAPAuthenticator.use_ssl = False
        c.LDAPAuthenticator.server_port = int(LDAP_PORT)
        if (LDAP_URI[0:8] == "ldaps://"):
          c.LDAPAuthenticator.use_ssl = True
        c.LDAPAuthenticator.bind_dn_template = 'uid={username},'+LDAP_BASE_DN

    else:
        print ("ERROR: Authentication type not specified.")
        print ("Cannot start JupyterHub.")


    ### Configuration for single-user containers ###
    #c.KubeSpawner.image_pull_policy = 'Always'
    # Spawn single-user's servers in the Kubernetes cluster
    c.JupyterHub.spawner_class = 'swanspawner.SwanKubeSpawner'
    c.SwanSpawner.image = CONTAINER_IMAGE
    c.SwanSpawner.namespace = NAMESPACE
    c.SwanSpawner.options_form = '/srv/jupyterhub/jupyterhub_form.html'
    c.SwanSpawner.start_timeout = 90

    # Single-user's servers extra config, CVMFS, EOS
    c.SwanSpawner.local_home = False	# $HOME is on EOS
    c.SwanSpawner.eos_path_prefix = EOS_USER_PATH

    c.SwanSpawner.available_cores = ["1", "2"]
    c.SwanSpawner.available_memory = ["2", "4"]
    c.SwanSpawner.check_cvmfs_status = False  # For now it only checks if available in same place as Jupyterhub.

    # local_home equal to true to hide the "always start with this config"
    c.SpawnHandlersConfigs.local_home = True
    c.SpawnHandlersConfigs.metrics_on = False  # For now the metrics are hardcoded for CERN
    c.SpawnHandlersConfigs.spawn_error_message = """SWAN could not start a session for your user, please try again. If the problem persists, please check:
    <ul>
        <li>Do you have a CERNBox account? If not, click <a href="https://cernbox.cern.ch" target="_blank">here</a>.</li>
        <li>Is there a problem with the service? Find information <a href="https://cern.service-now.com/service-portal/ssb.do" target="_blank">here</a>.</li>
        <li>If none of the options apply, please open a <a href="https://cern.service-now.com/service-portal/function.do?name=swan" target="_blank">Support Ticket</a>.</li>
    </ul>"""


    c.SwanSpawner.volume_mounts = [
        {
            'name': 'eos',
            'mountPath': '/eos',
            'mountPropagation': 'HostToContainer',
        },
        {
            'name': 'cvmfs-sft-cern-ch',
            'mountPath': '/cvmfs/sft.cern.ch',
        },
        {
            'name': 'tmp-volume',
            'mountPath': '/tmp',
        }
    ]

    c.SwanSpawner.volumes = [
        {
            'name': 'eos',
            'hostPath': {
                'path': '/var/eos'
            }
        },
        {
            'name': 'cvmfs-sft-cern-ch',
            'persistentVolumeClaim': {
                'claimName': 'cvmfs-sft-cern-ch-pvc'
            }
        },
        {
            'name': 'tmp-volume',
            'emptyDir': {}
        }
    ]


    c.SwanSpawner.port = 8888

    c.SwanSpawner.spark_ports_per_pod = 6


    def modify_pod_hook_call(spawner, pod):
        """
        :param spawner: todo
        :type spawner: kubespawner.KubeSpawner
        :param pod: todo
        :type pod: client.V1Pod
        :returns: todo
        :rtype: client.V1Pod
        """

        username = spawner.user.name
        user_tokens_secret = "user-tokens" + "-" + username
        spark_ports_service = "spark-ports" + "-" + username
        pod_label = {'swan': username}
        notebook_container = pod.spec.containers[0]

        # Add pod label for service selector
        pod.metadata.labels.update(
            pod_label
        )

        def append_or_replace_by_name(list, element):
            found = False
            for list_index in range(0, len(list)):
                spawner.log.warning(list[list_index])
                if list[list_index].to_dict().get("name") == element.to_dict().get("name"):
                    list[list_index] = element
                    found = True
                    break

            if not found:
                list.append(element)

            return list

        def create_notebook_ports():
            # TODO: ports can be probably moved to kubespawner as this is infrastructure independent
            # FIXME: this might be needed in case hadoop-yarn 2.8.0 webapp still crashes on ApplicationProxy UI
            #pod.spec.host_network = True

            try:
                spark_ports_env = []

                # Allocate some 6 random NodePorts on the cluster for spark
                service_template_ports = []
                for port_id in range(1, c.SwanSpawner.spark_ports_per_pod+1):
                    service_template_ports.append(
                        client.V1ServicePort(
                            name="spark-port-" + str(port_id),
                            port=port_id
                        )
                    )
                service_template = client.V1Service(
                    api_version="v1",
                    kind="Service",
                    metadata=client.V1ObjectMeta(
                        name=spark_ports_service
                    ),
                    spec=client.V1ServiceSpec(
                        selector=pod_label,  # attach this service to the pod with label {pod_label}
                        ports=service_template_ports,
                        type="NodePort"
                    )
                )

                try:
                    # use existing if possible
                    service = spawner.api.read_namespaced_service(spark_ports_service, NAMESPACE)
                except ApiException:
                    # not existing, create
                    service = spawner.api.create_namespaced_service(NAMESPACE, service_template)

                for port_id in range(len(service.spec.ports)):
                    # Adjust the service to map nodeport:targetport
                    name = service.spec.ports[port_id].name
                    node_port = service.spec.ports[port_id].node_port
                    service.spec.ports[port_id] = client.V1ServicePort(
                        name=name,
                        node_port=node_port,
                        port=node_port,
                        target_port=node_port
                    )

                    # Construct ports env for spark
                    spark_ports_env.append(str(node_port))

                    # Open proper ports in the notebook container to map nodeport:targetport
                    notebook_container.ports = append_or_replace_by_name(
                        notebook_container.ports,
                        client.V1ContainerPort(
                            name=name,
                            container_port=node_port
                        )
                    )
                spawner.api.replace_namespaced_service(spark_ports_service, NAMESPACE, service)

                # Add ports env for spark
                notebook_container.env = append_or_replace_by_name(
                    notebook_container.env,
                    client.V1EnvVar(
                        name='SPARK_PORTS',
                        value=','.join(spark_ports_env)
                    )
                )
                notebook_container.env = append_or_replace_by_name(
                    notebook_container.env,
                    client.V1EnvVar(
                        name='SERVER_HOSTNAME',
                        value=SERVER_HOSTNAME
                    )
                )
            except ApiException as e:
                raise Exception("Could not create required user ports: %s\n" % e)

        def create_swan_secrets():
            # Create eos token
            try:
                eos_token_base64 = subprocess.check_output(
                    ['sudo', '/srv/jupyterhub/private/eos-token.sh', username], timeout=60
                ).decode('ascii')
            except Exception:
                raise Exception("Could not create required user credential\n")

            # Create V1Secret with eos token
            try:
                secret_data = client.V1Secret()

                secret_meta = client.V1ObjectMeta()
                secret_meta.name = user_tokens_secret
                secret_meta.namespace = NAMESPACE
                secret_data.metadata = secret_meta
                secret_data.data = {}
                secret_data.data['eosToken'] = eos_token_base64

                try:
                    spawner.api.read_namespaced_secret(user_tokens_secret, NAMESPACE)
                    exists = True
                except ApiException:
                    exists = False

                if exists:
                    spawner.api.replace_namespaced_secret(user_tokens_secret, NAMESPACE, secret_data)
                else:
                    spawner.api.create_namespaced_secret(NAMESPACE, secret_data)
            except ApiException as e:
                raise Exception("Could not create required user secret: %s\n" % e)

            # Allocate V1 Secret volume to pod exlusivelly
            pod.spec.volumes.append(
                {
                    'name': user_tokens_secret,
                    'secret': {
                        'secretName': user_tokens_secret,
                        'items': [
                            {
                                'key': 'eosToken',
                                'path': 'krb5cc',
                            }
                        ],
                    }
                }
            )

            # Start side container which currently:
            #  - refreshes the kerberos token
            side_container = {
                "name": "side-ontainer",
                "image": "busybox",
                "command": [
                    "/bin/sh", "-c",
                    "while true; do " +
                    "cp /secrets/krb5cc /krb5cc_tmp; chmod 400 /krb5cc_tmp; chown $USER_ID:$USER_GID /krb5cc_tmp; mv /krb5cc_tmp /tmp/krb5cc_$USER_ID; sleep 60; " +
                    "done;"
                ],
                "env": [
                    {
                        "name": "USER_ID",
                        "value": str(pwd.getpwnam(username).pw_uid)
                    },
                    {
                        "name": "USER_GID",
                        "value": str(pwd.getpwnam(username).pw_gid)
                    }
                ],
                "volumeMounts": [
                    {
                        'name': user_tokens_secret,
                        'mountPath': '/secrets/',
                    },
                    {
                        'name': 'tmp-volume',
                        'mountPath': '/tmp',
                    }
                ]
            }

            pod.spec.containers = [
                side_container,  # start first
                notebook_container  # start second
            ]

        create_notebook_ports()
        create_swan_secrets()

        return pod


    c.SwanSpawner.modify_pod_hook = modify_pod_hook_call
  jupyterhub_form.complete.html: |-
    <style> .nb{ font-weight:normal } </style>
    <style> .nbs{ font-weight:normal; font-size:small } </style>
    <script type="text/javascript">
    <!--
       function toggle_visibility(id) {
          var e = document.getElementById(id);
          if(e.style.display == 'block')
             e.style.display = 'none';
          else
             e.style.display = 'block';
       }
    //-->
    </script>
    <br>
    <label for="placeholder">
    <span class='nb'>Specify the parameters that will be used to contextualise the container which is created for you. See <a target="_blank" href="https://swan.web.cern.ch/content/faq">the online SWAN guide</a> for more details.</span>
    </label>
    <br><br>
    <label for="LCG-rel">Software stack <a href="#" onclick="toggle_visibility('lcgReleaseDetails');"><span class='nbs'>more...</span></a>
    <div style="display:none;" id="lcgReleaseDetails">
       <span class='nb'>The software stack to associate to the container. See the <a target="_blank" href="http://lcginfo.cern.ch/">LCG package info</a> page.</span>
    </div>
    </label>
    <select name="LCG-rel">
       <option style="border-bottom:1px solid rgba(153,153,153,.3); margin:-10px 0 4px 0" disabled="disabled">Recommended releases (General Purpose)</option>
       <option value="LCG_96" selected="">96</option>
       <option value="LCG_96python3">96 Python3</option>
       <option style="border-bottom:1px solid rgba(153,153,153,.3); margin:-10px 0 4px 0" disabled="disabled"></option>
       <option style="border-bottom:1px solid rgba(153,153,153,.3); margin:-10px 0 4px 0" disabled="disabled">Development releases (might be unstable)</option>
       <option value="dev3/latest">Bleeding Edge</option>
       <option value="dev3python3/latest">Bleeding Edge Python3</option>
       <option style="border-bottom:1px solid rgba(153,153,153,.3); margin:-10px 0 4px 0" disabled="disabled"></option>
       <option style="border-bottom:1px solid rgba(153,153,153,.3); margin:-10px 0 4px 0" disabled="disabled">Development releases (GPU)</option>
       <option value="LCG_96py3cu10">Cuda 10 Python3</option>
       <option style="border-bottom:1px solid rgba(153,153,153,.3); margin:-10px 0 4px 0" disabled="disabled"></option>
       <option style="border-bottom:1px solid rgba(153,153,153,.3); margin:-10px 0 4px 0" disabled="disabled">Other releases</option>
       <option value="LCG_95a">95a</option>
       <option value="LCG_95apython3">95a Python3</option>
       <option value="LCG_94">94</option>
       <option value="LCG_94python3">94 Python3</option>
       <option value="LCG_93">93</option>
       <option value="LCG_93python3">93 Python3</option>
       <option value="LCG_92">92</option>
       <option value="LCG_92python3">92 Python3</option>
       <option value="LCG_91">91</option>
       <option value="LCG_91python3">91 Python3</option>
       <option value="LCG_90">90</option>
       <option value="LCG_90python3">90 Python3</option>
       <option value="LCG_89">89</option>
       <option value="LCG_89python3">89 Python3</option>
       <option value="LCG_88">88</option>
       <option value="LCG_88Py3">88 Python3</option>
       <option value="LCG_87">87</option>
       <option value="LCG_86">86</option>
       <option value="LCG_85swan3">85 SWAN3</option>
    </select>
    <br>
    <label for="platform">Platform <a href="#" onclick="toggle_visibility('platformDetails');"><span class='nbs'>more...</span></a>
    <div style="display:none;" id="platformDetails">
       <span class='nb'>The combination of compiler version and flags.</span>
    </div>
    </label>
    <select name="platform">
       <option value="x86_64-centos7-gcc8-opt" selected="">CentOS 7 (gcc8)</option>
       <option value="x86_64-centos7-gcc7-opt">CentOS 7 (gcc7)</option>
       <option value="x86_64-centos7-gcc62-opt">CentOS 7 (gcc62)</option>
    </select>
    <br>
    <label for="scriptenv">Environment script <a href="#" onclick="toggle_visibility('scriptenvDetails');"><span class='nbs'>more...</span></a>
    <div style="display:none;" id="scriptenvDetails">
       <span class='nb'>User-provided bash script to define custom environment variables. The variable CERNBOX_HOME is resolved to the proper /eos/user/u/username directory.</span>
    </div>
    </label>
    <input type="text" name="scriptenv" placeholder="e.g. $CERNBOX_HOME/MySWAN/myscript.sh">
    <br>
    <label for="ncores">Number of cores <a href="#" onclick="toggle_visibility('ncoresDetails');"><span class='nbs'>more...</span></a>
    <div style="display:none;" id="ncoresDetails">
       <span class='nb'>Number of cores to associate to the container.</span>
    </div>
    </label>
    <select name="ncores">
        <option value="1" selected>1</option>
        <option value="2">2</option>
    </select>
    <br>
    <label for="memory">Memory <a href="#" onclick="toggle_visibility('memoryDetails');"><span class='nbs'>more...</span></a>
    <div style="display:none;" id="memoryDetails">
       <span class='nb'>Amount of Memory allocated to the container.</span>
    </div>
    </label>
    <select name="memory">
        <option value="2" selected>2 GB</option>
        <option value="4">4 GB</option>
    </select>
    <br>
    <label for="spark-cluster">Spark cluster <a href="#" onclick="toggle_visibility('sparkClusterDetails');"><span class='nbs'>more...</span></a>
    <div style="display:none;" id="sparkClusterDetails">
       <span class='nb'>Name of the Spark cluster where to offload computations.</span>
    </div>
    </label>
    <select name="spark-cluster">
      <option value="none" selected>None</option>
      <option value="hadoop-qa">Hadoop QA</option>
    </select>
---
apiVersion: v1
kind: Secret
metadata:
  name: &name swan-tokens
  namespace: &namespace swan
stringData:
  eos-token.sh: |-
    #!/bin/bash
    set -e

    # 1. Go to container with:
    #   kubectl exec -it -n swan $(kubectl get pods -n swan | grep swan- | grep Running | awk '{print $1}') bash)
    # 2. kinit to /tmp/krb5cc_0
    # 3. echo eos token

    if [[ ! -f "/tmp/krb5cc_0" ]]; then
        exit 1;
    fi

    echo $(cat /tmp/krb5cc_0 | base64 -w 0)